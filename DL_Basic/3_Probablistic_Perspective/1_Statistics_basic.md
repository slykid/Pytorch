# 0. 들어가며
- 신경망을 학습하는 이유는?
  - 가상의 함수를 모사하여, 원하는 출려 값을 반환하는 신경망 파라미터 찾기
<br><br>
    
- 이 세상은 모두 확률에 기반한다!
  - 어떤 내용에 대해서 모두가 다 하나의 정답을 생각하지 않는다!
  - 때문에 신경망을 학습하는 목표는 정확히 말하자면,<b>확률분포</b>를 학습하는 것이다.
<br><br>
    
- 무슨 차이인가?
  - 함수를 모사하자<br>
    = 정답에 최대한 근사하게 모사하자!
<br><br>
    
  - 확률분포를 배우자<br>
    = 수학적으로 좀 더 설명가능하기 때문에, 불확실성까지 학습하자!
    
# 1. 기본 확률 통계  
## 1) Random Variable & Probability Distribution
- 확률 변수란?
  - 입력 x 가 $x$ 라는 값을 가질 확률
<br><br>

- 확률 값의 총합은 반드시 <b>1</b>이다.
<br><br>

- 확률 질량 함수
  - 특정 샘플이 주어졌을 때, x 라는 값이 나올 확률에 대한 함수
  - 이산적으로 그래프가 생김(=특정 지점에서의 확률 값으로 나옴)
<br><br>
   
- 확률 밀도 함수
  - 면적의 합이 1이다
  - 밀도 함수이기 때문에 함수의 출력은 1보다 클 수 있다.
  - 연속 확률 변수의 경우, 특정 샘플이 주어졌을 때의 확률 값은 알 수 없다. 
    - 특정 구간의 면적이 확률 값임
    - 특정 지점의 함수 값은 0임(특정 지점이고, 그에 대한 면적은 0이기 때문)
<br><br>

- 결합분포
  - 2개 이상의 확률 분포가 결합되어 생기는 분포
<br><br>

- 조건부 확률 분포
  - x의 조건이 주어질 때, y 가 나올 확률의 분포
<br><br>
   

- 베이지안 이론
  - 데이터 D가 주어졌을 때, 가설 h의 확률

## 2) MLE (Maximum Likelihood Estimation)
- Likelihood 란?
  - 입력으로 주어진 확률분포(샘플의 기댓값, 표준편차) 가 데이터를 얼마나 잘 설명하는가를 나타내는 점수에 대한 함수
  - 데이터를 얼마나 잘 설명하는 지를 나타내는 지표
<br><br>
    
- MLE
  - Likelihood 값이 최대화가 되는 추정 방법
  - 데이터를 얼마나 잘 설명하는 지에 대한 점수를 최대화 시키는 방법을 추정하는 기법<br>
    → <b>데이터를 가장 잘 설명하는 파라미터를 추정하는 기법</b>
  - 값이 너무 작어 underflow 가 발생할 수 있기 때문에 <b>Log-Likelihood Estimation</b> 이 등장함 
<br><br>
    
- 경사하강법으로 접근?
  - 정확히는 경사상승법 (Gradient Ascent) 으로 찾을 수 있다!
    - 왜 Ascent 인가?<br>
      확률 밀도 함수는 위로 볼록한 함수이며, 우리가 찾고자 하는 최적점은 가장 볼록한 부분(순간기울기=0 인 지점) 이기 때문에, 그레디언트가 상승하는 방향으로 조금씩 이동해야된다!
      
# 2. 신경망과 MLE의 관계
## 1) Review
- 신경망의 목표
  - 확률분포로부터 샘플리하여 데이터를 넣을 때, 확률 분포를 반환하는 가상의 함수를 모사하는 것이다.
  - 그 때의 파라미터를 구하는 것이 최종 목표임
<br><br>
    
- Maximum Likelihood Estimation
  - 목표 확률 분포로부터 데이터를 수집한 후, 데이터를 가장 잘 설명하는 것을 찾는 추정기법

## 2) 딥러닝의 확률분포
- 그렇다면 딥러닝에서의 확률 분포는?
  - 신경망을 구성하는 가중치(W) 파라미터에 해당한다!
    - 신경망도 하나의 <b>확률분포함수</b> 이기 때문
<br><br>      

  - 마찬가지로 Gradient Ascent 를 통해 Likelihood 를 최대화 하는 파라미터를 찾는다!

## 3) NLL (Negative Log Likelihood)    
- but, 대부분의 딥러닝 프레임워크들은 Gradient Descent 를 지원한다. 
- 그럼 Gradient Ascent 의 구현은?
  - Likelihood 에 음수를 취해주자! ("음수 x 음수 = 양수" 니까)
<br><br>
    
- 위의 내용을 위해 NLL의 경우, 값이 작아야만, 데이터를 잘 설명한다. 
  - Gradient Descent 를 통해 NLL 을 최소화하는 파라미터 값을 찾는다.
<br><br>
    
- 위의 내용을 수행하기 위한 과정 = 역전파(Backpropagation)
    

    


    